%\vspace{-20pt}
\section*{Abstract}
The growth in SSD capacity is reaching its limit 
due to the stunted growth of capacitors---electrical components that store charge 
to protect data for the volatile memory in case of power loss. 
While the SSD's capacity scaled from 256GB in 2011~\cite{samsung2011} to 30TB in 2018~\cite{anandtech18samsung} (100$\times$), 
the density of Aluminum and Tantalum electrolytic capacitors only by tenfold from 1960 to 2005~\cite{both2015modern}.
This slow scaling will eventually limit the amount of DRAM that can be used in an SSD,
and this, in turn, will also limit the storage capacity as the size of DRAM and aggregate flash capacity proportionally scale~\cite{samsung_ratio, ni2017hash}. 

\iffalse
Enterprise-class SSDs provide a persistent internal buffer using the capacitors. 
This PLP(Power-Loss Protection) mechanism reached a limit as the SSD density outpaces the scaling capability of the capacitors. 
% The SSD has increased significantly in density for the past decade. 
% With the growing popularity of data-intensive applications, the SSD density has increased rapidly in recent years.
%, expanding by 100× over the past ten years [1], [15].
% As an example, 
In 2011, a typical 2.5-inch SSD had 256GB capacity, but
%the world's first 2TB SSD was released in 2013~\cite{foremay2013}, but 
by 2018, a high-capacity SSD boasted a 30TB, expanding by 100× over the past ten years
~\cite{samsung2011, anandtech18samsung}. 
\begin{comment}
This remarkable growth of the device-capacity is thanks to the advanced scaling technologies 
such as nanoscale fabrication~\cite{busche2014design} and multi-layer stacking~\cite{9365809}. 
\end{comment}
% As an example, in 2011, a typical 2.5-inch SSD had 256GB capacity with 256MB of DRAM; by 2018, a high-capacity 2.5- inch SSD boasted a 30TB with 40GB of
% Unfortunately, not all components of the SSDs have kept up with the scaling rate.
However, the capacitor fails to proceed at the pace. 
\begin{comment}
Historically, storage devices 
have been equipped with a small size of volatile buffer in front of the persistent disk. 
By using them as a write buffer, they hide a long latency of the physical storage medium 
as well as mitigating an endurance limitation of the worn-out devices. 
However, the volatile buffer loses all data in the event of power crash. 
To prevent a data loss or corruption by this, enterprise-class SSDs
rely on the capacitors; it reserves energy to persist data in volatile buffer 
in the unforeseen event of a power crash. 
In addition, the adoption of capacitors enables an SSD to ignore the \texttt{FLUSH} command that explicitly requests all data in the volatile buffer to be made durable.
This property increases the buffering effect in SSD significantly, leading to both less write traffic and a shorter operation latency.
% To overcome this limitation without sacrificing performance, 
The reliance on capacitors, however, has reached its limit. 
\end{comment}
% the improvement in capacitance fails to keep up with the rapid growth of SSDs. 
%Al(aluminum) and Ta(tantalum)-electrolytic capacitors used in SSDs have increased in density through miniaturization by tenfold from 1960 to 2005 [4]. 
The capacitance density has also steadily improved, but
it is not as rapid as the SSD scaling speed. 
Al(aluminum) and Ta(tantalum)-electrolytic capacitors used in SSDs 
have increased in density by tenfold from 1960 to 2005~\cite{both2015modern}. 
This is approximately 50x slower than the SSD density increase rate.
Given that the internal buffer size increases in proportion to the storage capacity (typically 0.1\% of storage capacity~\cite{samsung_ratio, ni2017hash}),
the density gap between capacitance and memory technologies 
imposes an intrinsic limitation on the current architecture wherein 
the entire buffer is protected by capacitors. 
\fi

This paper presents \ours{}, a novel SSD-internal DRAM management scheme 
that allows the SSD capacity to scale beyond the slow growth of capacitors. 
SSD-internal DRAM is used for 
(1) caching translation information (also known as mapping table) and (2) buffering user writes. 
In typical SSD designs, most of the capacitance is used for protecting the mapping table (to keep as many translation entries in DRAM) 
and the buffer for user writes is kept at a minimal (just enough to hide the flash program latency)~\cite{KangLMKO14sigmod}. 
However, in our design, we take a radically different approach. 
We buffer more user writes so that mapping entry eviction becomes more efficient by aggregating dirty updates. 
This substantially reduces the amount of mapping table-related write traffic, and in turn, improves the overall performance. 

\iffalse
This paper presents a device-internal buffer architecture called \ours{} 
for the SSDs under capacitance constraints. 
% which operates under capacitance constraints.
% Fig.~\ref{fig_dawid_archi} shows the SSD architecture targeted in our study. 
\fi
\iffalse
\textcolor{blue}{
Historically, storage devices have been equipped with a small size of volatile buffer in front of the persistent disk. By using them as a read cache and a write buffer, they hide a long latency of the physical storage medium as well as mitigating an endurance limitation of the worn-out devices.
However, the volatile buffer loses all data in the event of power crash. 
To prevent a data loss or corruption by this, enterprise-class SSDs
rely on the capacitors; it reserves energy to persist data in volatile buffer 
in the unforeseen event of a power crash. 
In addition, the adoption of capacitors enables an SSD to ignore the \texttt{FLUSH} command that explicitly requests all data in the volatile buffer to be made durable.
This property increases the buffering effect in SSD significantly, leading to both less write traffic and a shorter operation latency.
% \EUNJI{Need to make this part shorter}
}
\fi
% To overcome this limitation without sacrificing performance, 
% The reliance on capacitors, however, has reached its limit. 
\iffalse
\ours{} achieves a persistent buffer with small size of capacitance by answering the following two questions. 
%(1) what not to protect under capacitance limitation and (2) how to reduce the overhead of ensuring persistence for unprotected data.
First question is \textit{what not to protect under capacitance constraints}. The device-internal buffer is used for (1) caching translation information (i.e., mapping index) and (2) buffering user writes. 
% The data maintained in the buffer can be classified into two types: the actual user data and 
% the metadata for SSD management (i.e, mapping table). 
\ours{} applies the capacitance constraints only to translation information, while protecting the user data entirely. The user data write is synchronous with the user request and unrecoverable upon a power outage. It hampers user experiences seriously when unprotected. On contrary, 
translation information is entirely managed by the firmware and provides room for compromise when SSDs suffer from capacitance restriction. \EUNJI{Furthermore, the main culprit demanding buffer space is translation information because a flat structured table is widely used for its indexing and has a large memory footprint.} 
Second question is thus \textit{how to reduce the cost of ensuring persistence for the translation updates}. When the buffer is partially protected, the number of dirty pages is limited to the maximum amount of data that the on-board capacitance can protect. If it goes beyond the limit, changes should be immediately flushed to the flash memory. \ours{} mitigates the negative impact of this behavior with a capacitance-contraint aware I/O scheduling.
%to meet the durability constraint for SSDs. XX mapping table XX flush 줄여보자.

% otherwise durability violated. 
% \EUNJI{
% 보호되지 못하는 데이터가 있을 때 버퍼를 어떻게 관리하면, 보호되지 못하는 데이터의 persistence 를 유지하기 위해 발생하는 비용을 적게 발생시킬 것인가? 에 대한 질문. }
% \EUNJI{제안하는 기법: 부분적으로 보호되는 맵핑 테이블의 동기화 오버헤드를 줄이기 위해, 해당 비용을 최소로 증가시키도록 I/O 스케줄링을 하는 것임.}
\fi
%\section{Design of \ours{} Buffer}
% \section{Least Increase of Dirtiness Scheduling}
\iffalse
A primary way of  reducing the superfluous writes caused by a capacitance limitation is to maintain the dirty memory footprint below a protected threshold as long as possible. When a dirty page overflow occurs, flushing is forced and write amplification increases.
% The \ours{} buffer aims at minimizing the dirty memory footprint of the mapping table at any point in time.
To this end, \ours{} aggregates and batches the requests whose mapping entry resides in the same page. This policy reduces the dirty memory footprint of the mapping table at any point in time, amortizing the synchronization overhead across more requests. As such, \ours{} delivers less write traffic and higher IOPS than existing scheduling (e.g., FIFO) under capacitance constraints. 
\fi

To realize this design, 
\ours{} maintains two data structures: first, \textit{a zero-cost list} 
that holds the write requests whose mapping entry is already in a dirty translation page, 
and second, \textit{a max binary heap} that maintains the indexes to translation pages
sorted by the number of buffered user write requests associated with that page. 
% We term this policy Least Increase of Dirtiness (LID) scheduling.
When there is sufficient bandwidth at underlying NAND flash subsystem for writes, 
\ours{} first flushes user data from the  zero-cost list,
and then persists the dirty translation pages as ordered by the max binary heap. 
By doing so, each user write minimizes the number of eventual translation page write, 
and each translation page write maximizes the number of persisted mapping entries. 
\iffalse
\BSK{What's policy on the maximum number of dirty translation pages? Dawid should also flush when that condition satisfies, right?}
\EUNJI{FIFO로 flush 합니다. SSD 가 한번에 병렬적으로 쓸 수 있는 단위씩 보냅니다.}
\fi
 

\iffalse
In this regard, the \ours{} buffer aims at minimizing the \textit{dirty memory footprint} of the mapping table at any point in time. To this end, \ours{} processes outstanding requests in the order that least increases the number of dirty pages in the mapping table. 
This scheme not only reduces the number of flushes for the mapping table partially protected but also increases the efficiency of flush operation by aggregating more translation updates into the smaller translation pages.
\fi

\iffalse
because (1) the data write is synchronous with the user request and (2) the user data takes up a relatively 
small footprint in the buffer. 
\fi
% This is because the storage suffers from serious performance degradation when the user data 
% is not fully protected and should be entirely flushed upon a \texttt{FLUSH} command.
% }
% \textcolor{brown}{
% \ours{} essentially limits the number of dirty pages within buffer
% to the level the maximum capacitance can protect.
% If the number of dirty pages goes beyond the limit, changes are flushed to the flash memory. 
% }
% \EUNJI{For this architecture, the problem boils down to how to reduce the write traffic to persist the mapping table to flash memory under capacitance constraints.
% \iffalse
\iffalse
Reducing the synchronization overhead of the mapping table is a well-known problem and has been extensively studied for a past decade~\cite{jiang2011s, kim2017shrd}. 
However, they mostly focus on the SSDs without PLP, which have different properties to the PLP-SSDs with capacitance constraints. 
\fi

\ours{} is built upon the current trend of increasing the queue depth of the storage interfaces. SATA and SAS support a single queue with 32 and 245 commands, but NVMe has 
up to 65,535 queues with as many as 65,536 commands per queue. 
This extension allows SSDs to further optimize the internal activities by taking advantage of the outstanding request information.


\begin{figure}[t]
    \centering{}
    \subfloat[Write Traffic] {
	    \includegraphics[width=0.4\textwidth]{figure/all-wt.eps}
	} \\
	\subfloat[IOPS] {
	    \includegraphics[width=0.4\textwidth]{figure/all-iops.eps}
	}
	\caption{Fio benchmark results.}
	\label{fig_fio}
\end{figure}

We implement \ours{} in \texttt{FEMU}, an open-source SSD development framework~\cite{li2018case}.
We assume 1\% of the mapping table is protected via capacitors in a 64GB SSD. 
%The 64GB SSD is using DRAM and assumes that 1\% of the mapping table is protected. 
We measured the performance of \ours{} using fio benchmark~\cite{fio-bench},
running 4KB sequential writes, 4KB random writes, 
and the skewed read-write mixed workload that follows JESD219 using 8 threads. 
A total of 90GB of data was written to the 30GB area.

\EUNJI{
Figure~\ref{fig_fio} compares IOPS and the write traffic of FIFO buffer management and \ours{}.
For sequential writes, there are no prominent differences between FIFO and \ours{}. 
However, \ours{} reduces the write traffic by 35\% and 46\% on average 
for random writes and JESD219 workloads when the buffer size is 4MB and 4GB, respectively. 
These workloads have a large footprint at a time window and thus buffering and scheduling them judiciously leads to a large reduction in mapping table persistence overhead.
Consequently, IOPS of our design improves up to 18\% and 88\% for random write and JESD219, respectively.
}

% \EUNJI{
% To extending this work, we plan to investigate the trade-offs between performance and capacitance.
% }

\iffalse
To evaluate the effectiveness of \ours, we implement the proposed buffer design
in \texttt{FEMU}, which is an open-source SSD development framework~\cite{li2018case}. The performance evaluation with various workloads 
shows that \ours{} reduces the write traffic by up to 78\% and provides 25\% higher IOPS 
compared to the FIFO scheduling scheme when only 10\% of the mapping table is protected. 
Compared to the full-protection architecture, \ours{} has has 20\% more writes and 
9\% of performance overhead, while reducing the required capacitance by 90\%. 
\fi

% \begin{figure}[t]
%     \centering{}
%     \includegraphics[width=0.20\textwidth]{shn-graph/rand-wt.eps}
%     \includegraphics[width=0.20\textwidth]{shn-graph/rand-iops.eps}
%     %\includegraphics[width=0.4\textwidth]{figure/dawid_ssd_archi.png}
%     \caption{\textbf{Write Traffic and IOPS (FIO-RAND).}}
%     \label{fig_dawid_archi}
% \end{figure}

% \begin{figure}[t]
%     \centering{}
%     \includegraphics[width=0.20\textwidth]{shn-graph/jesd-wt.eps}
%     \includegraphics[width=0.20\textwidth]{shn-graph/jesd-iops.eps}
%     %\includegraphics[width=0.4\textwidth]{figure/dawid_ssd_archi.png}
%     \caption{\textbf{Write Traffic and IOPS (FIO-JESD) .}}
%     \label{fig_dawid_archi}
% \end{figure}

